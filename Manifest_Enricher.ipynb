{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOWJB+TgEvYmQa/ZkDR1Znr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsandaver/hsandaver/blob/main/Manifest_Enricher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSVP1NTxJ2n0"
      },
      "outputs": [],
      "source": [
        "# ============================== #\n",
        "#      Enhanced Python Script    #\n",
        "# ============================== #\n",
        "\n",
        "# ==============================\n",
        "# 1. Install Required Libraries\n",
        "# ==============================\n",
        "\n",
        "# Uncomment the following lines to install required libraries if not already installed.\n",
        "# You can run this cell separately to install the dependencies.\n",
        "\n",
        "# !pip install requests\n",
        "# !pip install networkx\n",
        "# !pip install plotly\n",
        "# !pip install ipywidgets\n",
        "# !pip install fuzzywuzzy\n",
        "# !pip install python-Levenshtein  # Optional for improved fuzzy matching performance\n",
        "# !pip install unidecode\n",
        "# !pip install cachetools\n",
        "# !pip install jsonschema\n",
        "\n",
        "# ==============================\n",
        "# 2. Import Necessary Libraries\n",
        "# ==============================\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import networkx as nx\n",
        "import plotly.graph_objs as go\n",
        "from ipywidgets import (\n",
        "    widgets, VBox, HBox, Layout, Button, Textarea, IntSlider, Output, HTML\n",
        ")\n",
        "from IPython.display import display, clear_output\n",
        "import plotly.io as pio\n",
        "import logging\n",
        "from fuzzywuzzy import fuzz\n",
        "from urllib.parse import urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from unidecode import unidecode\n",
        "from cachetools import TTLCache, cached\n",
        "from jsonschema import validate, ValidationError\n",
        "\n",
        "# ==============================\n",
        "# 3. Configure Logging\n",
        "# ==============================\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# 4. Set Plotly Renderer\n",
        "# ==============================\n",
        "\n",
        "pio.renderers.default = 'colab'\n",
        "\n",
        "# ==============================\n",
        "# 5. Setup Caching\n",
        "# ==============================\n",
        "\n",
        "# Cache fetched data to avoid redundant network requests\n",
        "# Cache size: 100 items, TTL: 1 hour\n",
        "cache = TTLCache(maxsize=100, ttl=3600)\n",
        "\n",
        "# ==============================\n",
        "# 6. Define JSON Schemas\n",
        "# ==============================\n",
        "\n",
        "# Minimal JSON Schemas for validation\n",
        "IIIF_MANIFEST_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"@id\": {\"type\": \"string\"},\n",
        "        \"label\": {\"type\": [\"string\", \"object\"]},\n",
        "        \"metadata\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"label\": {\"type\": \"string\"},\n",
        "                    \"value\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"label\", \"value\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"@id\", \"label\", \"metadata\"]\n",
        "}\n",
        "\n",
        "LINKED_DATA_SCHEMA = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"@context\": {\"type\": \"string\"},\n",
        "        \"id\": {\"type\": \"string\"},\n",
        "        \"prefLabel\": {\"type\": \"object\"},\n",
        "        \"altLabel\": {\"type\": \"object\"},\n",
        "        \"dateOfBirth\": {\"type\": \"array\"},\n",
        "        \"dateOfDeath\": {\"type\": \"array\"},\n",
        "        \"description\": {\"type\": \"object\"}\n",
        "    },\n",
        "    \"required\": [\"@context\", \"id\", \"prefLabel\", \"altLabel\", \"dateOfBirth\", \"dateOfDeath\", \"description\"]\n",
        "}\n",
        "\n",
        "# ==============================\n",
        "# 7. Helper Functions\n",
        "# ==============================\n",
        "\n",
        "def validate_uri(uri):\n",
        "    \"\"\" Validate URI format before making network requests. \"\"\"\n",
        "    try:\n",
        "        result = urlparse(uri)\n",
        "        return all([result.scheme, result.netloc])\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def fetch_with_retries(session, uri, headers=None, max_retries=3, backoff_factor=0.3):\n",
        "    \"\"\"\n",
        "    Fetch data from a URI with retry mechanism.\n",
        "\n",
        "    Args:\n",
        "        session (requests.Session): Session object for connection pooling.\n",
        "        uri (str): The URI to fetch data from.\n",
        "        headers (dict): HTTP headers to include in the request.\n",
        "        max_retries (int): Maximum number of retries.\n",
        "        backoff_factor (float): Backoff factor for retries.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: JSON response if successful, else None.\n",
        "    \"\"\"\n",
        "    from requests.adapters import HTTPAdapter\n",
        "    from urllib3.util.retry import Retry\n",
        "\n",
        "    retry_strategy = Retry(\n",
        "        total=max_retries,\n",
        "        backoff_factor=backoff_factor,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        allowed_methods=[\"GET\"]  # Changed from 'method_whitelist' to 'allowed_methods'\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    session.mount(\"https://\", adapter)\n",
        "    session.mount(\"http://\", adapter)\n",
        "\n",
        "    try:\n",
        "        response = session.get(uri, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Request failed for {uri}: {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        logging.error(f\"Failed to decode JSON from {uri}\")\n",
        "        return None\n",
        "\n",
        "@cached(cache)\n",
        "def fetch_linked_data(uri):\n",
        "    \"\"\"Fetch linked data entity from a given URI with caching and validation.\"\"\"\n",
        "    if not validate_uri(uri):\n",
        "        logging.error(f\"Invalid URI: {uri}\")\n",
        "        return None\n",
        "\n",
        "    with requests.Session() as session:\n",
        "        data = fetch_with_retries(session, uri, headers={'Accept': 'application/ld+json'})\n",
        "        if data:\n",
        "            try:\n",
        "                validate(instance=data, schema=LINKED_DATA_SCHEMA)\n",
        "                logging.info(f\"Successfully fetched and validated linked data from {uri}\")\n",
        "                return data\n",
        "            except ValidationError as ve:\n",
        "                logging.error(f\"Linked data from {uri} failed validation: {ve.message}\")\n",
        "        return None\n",
        "\n",
        "@cached(cache)\n",
        "def fetch_iiif_manifest(uri):\n",
        "    \"\"\"Fetch IIIF manifest from a given URI with caching and validation.\"\"\"\n",
        "    if not validate_uri(uri):\n",
        "        logging.error(f\"Invalid URI: {uri}\")\n",
        "        return None\n",
        "\n",
        "    with requests.Session() as session:\n",
        "        data = fetch_with_retries(session, uri)\n",
        "        if data:\n",
        "            try:\n",
        "                validate(instance=data, schema=IIIF_MANIFEST_SCHEMA)\n",
        "                logging.info(f\"Successfully fetched and validated IIIF manifest from {uri}\")\n",
        "                return data\n",
        "            except ValidationError as ve:\n",
        "                logging.error(f\"IIIF manifest from {uri} failed validation: {ve.message}\")\n",
        "        return None\n",
        "\n",
        "def normalize_name(name):\n",
        "    \"\"\"\n",
        "    Normalize the name by:\n",
        "    - Transliteration (removing accents)\n",
        "    - Converting 'Last, First' to 'First Last'\n",
        "    - Lowercasing\n",
        "    - Stripping whitespace\n",
        "    \"\"\"\n",
        "    name = unidecode(name)\n",
        "    if ',' in name:\n",
        "        parts = name.split(',')\n",
        "        if len(parts) == 2:\n",
        "            first = parts[1].strip()\n",
        "            last = parts[0].strip()\n",
        "            name = f\"{first} {last}\"\n",
        "    return name.strip().lower()\n",
        "\n",
        "def extract_artist_info(linked_data_entity):\n",
        "    \"\"\"\n",
        "    Extract artist information from a linked data entity.\n",
        "\n",
        "    Args:\n",
        "        linked_data_entity (dict): The linked data JSON object.\n",
        "\n",
        "    Returns:\n",
        "        dict: Extracted artist information.\n",
        "    \"\"\"\n",
        "    artist_info = {}\n",
        "\n",
        "    # Extract preferred names from all English-related keys\n",
        "    pref_label = linked_data_entity.get('prefLabel', {})\n",
        "    preferred_names = []\n",
        "    for key in pref_label:\n",
        "        if key.startswith('en'):\n",
        "            preferred_names.append(pref_label[key])\n",
        "    if not preferred_names:\n",
        "        # Fallback to any available language\n",
        "        preferred_names.append(next(iter(pref_label.values()), 'Unknown'))\n",
        "    # Assuming the first preferred name is the primary one\n",
        "    artist_info['preferred_name'] = normalize_name(preferred_names[0])\n",
        "\n",
        "    # Extract alternative names from all English-related keys\n",
        "    alt_labels = linked_data_entity.get('altLabel', {})\n",
        "    alternative_names = []\n",
        "    for key, labels in alt_labels.items():\n",
        "        if key.startswith('en'):\n",
        "            if isinstance(labels, list):\n",
        "                alternative_names.extend([normalize_name(label) for label in labels])\n",
        "            else:\n",
        "                alternative_names.append(normalize_name(labels))\n",
        "    artist_info['alternative_names'] = alternative_names\n",
        "\n",
        "    # Extract date of birth\n",
        "    dob_entry = linked_data_entity.get('dateOfBirth', [{}])[0]\n",
        "    dob = dob_entry.get('time:inXSDDateTimeStamp', {}).get('@value', 'Unknown')\n",
        "    artist_info['date_of_birth'] = dob[:10] if dob != 'Unknown' else dob\n",
        "\n",
        "    # Extract date of death\n",
        "    dod_entry = linked_data_entity.get('dateOfDeath', [{}])[0]\n",
        "    dod = dod_entry.get('time:inXSDDateTimeStamp', {}).get('@value', 'Unknown')\n",
        "    artist_info['date_of_death'] = dod[:10] if dod != 'Unknown' else dod\n",
        "\n",
        "    # Extract descriptions from all English-related keys\n",
        "    descriptions = linked_data_entity.get('description', {})\n",
        "    description_texts = []\n",
        "    for key in descriptions:\n",
        "        if key.startswith('en'):\n",
        "            description_texts.append(descriptions[key])\n",
        "    if not description_texts:\n",
        "        # Fallback to any available language\n",
        "        description_texts.append(next(iter(descriptions.values()), 'No description available.'))\n",
        "    artist_info['description'] = description_texts[0]\n",
        "\n",
        "    artist_info['id'] = linked_data_entity.get('id', 'Unknown')\n",
        "\n",
        "    logging.debug(f\"Extracted Artist Info: {artist_info}\")\n",
        "    return artist_info\n",
        "\n",
        "def remove_duplicate_metadata(metadata_list):\n",
        "    \"\"\"\n",
        "    Remove duplicate metadata entries based on 'label' and 'value'.\n",
        "\n",
        "    Args:\n",
        "        metadata_list (list): List of metadata dictionaries.\n",
        "\n",
        "    Returns:\n",
        "        list: List with duplicates removed.\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    unique_metadata = []\n",
        "    for item in metadata_list:\n",
        "        key = (item['label'].lower(), item['value'].lower())\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            unique_metadata.append(item)\n",
        "    return unique_metadata\n",
        "\n",
        "def enrich_iiif_manifests(iiif_manifests, artists_info, fuzzy_threshold=85):\n",
        "    \"\"\"\n",
        "    Enrich IIIF manifests with artist information.\n",
        "\n",
        "    Args:\n",
        "        iiif_manifests (list): List of IIIF manifest dictionaries.\n",
        "        artists_info (list): List of artist information dictionaries.\n",
        "        fuzzy_threshold (int): Threshold for fuzzy matching.\n",
        "\n",
        "    Returns:\n",
        "        list: Enriched IIIF manifests.\n",
        "    \"\"\"\n",
        "    enriched_manifests = []\n",
        "    for manifest in iiif_manifests:\n",
        "        manifest_metadata = manifest.get('metadata', [])\n",
        "        manifest_id = manifest.get('@id', 'Unknown ID')\n",
        "        manifest_label = manifest.get('label', 'Unknown Label')\n",
        "        logging.info(f\"Processing Manifest: {manifest_label} ({manifest_id})\")\n",
        "\n",
        "        # Track if the manifest has been enriched to avoid multiple enrichments\n",
        "        enriched = False\n",
        "\n",
        "        # Iterate over each artist to check for matches\n",
        "        for artist_info in artists_info:\n",
        "            artist_names = [artist_info['preferred_name']] + artist_info['alternative_names']\n",
        "            normalized_artist_names = [normalize_name(name) for name in artist_names]\n",
        "            logging.debug(f\"Checking Artist: {artist_info['preferred_name']} with names {artist_names}\")\n",
        "\n",
        "            # Iterate over manifest metadata to find matching creator/artist/author\n",
        "            for item in manifest_metadata:\n",
        "                label = item.get('label', '').strip().lower()\n",
        "                value = item.get('value', '')\n",
        "                if label in ['creator', 'artist', 'author']:\n",
        "                    normalized_value = normalize_name(value)\n",
        "                    # Check for exact match\n",
        "                    if any(name == normalized_value for name in normalized_artist_names):\n",
        "                        logging.info(f\"Exact match found: '{value}' matches artist '{artist_info['preferred_name']}'\")\n",
        "                        # Enrich the manifest\n",
        "                        new_metadata_entries = [\n",
        "                            {'label': 'Enriched: Artist Preferred Name', 'value': artist_info['preferred_name']},\n",
        "                            {'label': 'Enriched: Artist Alternative Names', 'value': ', '.join(artist_info['alternative_names']) if artist_info['alternative_names'] else 'N/A'},\n",
        "                            {'label': 'Enriched: Artist Date of Birth', 'value': artist_info['date_of_birth']},\n",
        "                            {'label': 'Enriched: Artist Date of Death', 'value': artist_info['date_of_death']},\n",
        "                            {'label': 'Enriched: Artist Description', 'value': artist_info['description']},\n",
        "                            {'label': 'Enriched: Artist Linked Data ID', 'value': artist_info['id']}\n",
        "                        ]\n",
        "                        manifest['metadata'].extend(new_metadata_entries)\n",
        "                        # Remove duplicates\n",
        "                        manifest['metadata'] = remove_duplicate_metadata(manifest['metadata'])\n",
        "                        logging.info(f\"Enriched manifest '{manifest_label}' with artist info '{artist_info['preferred_name']}'\")\n",
        "                        enriched = True\n",
        "                        break  # Stop checking after enriching with one artist\n",
        "                    else:\n",
        "                        # If no exact match, perform fuzzy matching\n",
        "                        for name in normalized_artist_names:\n",
        "                            similarity = fuzz.partial_ratio(name, normalized_value)\n",
        "                            if similarity >= fuzzy_threshold:\n",
        "                                logging.info(f\"Fuzzy match found: '{value}' matches artist '{artist_info['preferred_name']}' with similarity {similarity}\")\n",
        "                                # Enrich the manifest\n",
        "                                new_metadata_entries = [\n",
        "                                    {'label': 'Enriched: Artist Preferred Name', 'value': artist_info['preferred_name']},\n",
        "                                    {'label': 'Enriched: Artist Alternative Names', 'value': ', '.join(artist_info['alternative_names']) if artist_info['alternative_names'] else 'N/A'},\n",
        "                                    {'label': 'Enriched: Artist Date of Birth', 'value': artist_info['date_of_birth']},\n",
        "                                    {'label': 'Enriched: Artist Date of Death', 'value': artist_info['date_of_death']},\n",
        "                                    {'label': 'Enriched: Artist Description', 'value': artist_info['description']},\n",
        "                                    {'label': 'Enriched: Artist Linked Data ID', 'value': artist_info['id']}\n",
        "                                ]\n",
        "                                manifest['metadata'].extend(new_metadata_entries)\n",
        "                                # Remove duplicates\n",
        "                                manifest['metadata'] = remove_duplicate_metadata(manifest['metadata'])\n",
        "                                logging.info(f\"Enriched manifest '{manifest_label}' with artist info '{artist_info['preferred_name']}' via fuzzy matching\")\n",
        "                                enriched = True\n",
        "                                break  # Stop checking after enriching with one artist\n",
        "                if enriched:\n",
        "                    break  # Move to next manifest after enrichment\n",
        "        enriched_manifests.append(manifest)\n",
        "    return enriched_manifests\n",
        "\n",
        "def create_relationship_graph(iiif_manifests, artists_info):\n",
        "    \"\"\"\n",
        "    Create and display a relationship graph between artists and artworks.\n",
        "\n",
        "    Args:\n",
        "        iiif_manifests (list): List of enriched IIIF manifest dictionaries.\n",
        "        artists_info (list): List of artist information dictionaries.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add artist nodes\n",
        "    for artist_info in artists_info:\n",
        "        G.add_node(artist_info['preferred_name'], type='artist', description=artist_info['description'])\n",
        "        logging.debug(f\"Added artist node: {artist_info['preferred_name']}\")\n",
        "\n",
        "    # Add artwork nodes and edges\n",
        "    for manifest in iiif_manifests:\n",
        "        manifest_label = manifest.get('label', 'Unknown')\n",
        "        G.add_node(manifest_label, type='artwork')\n",
        "        logging.debug(f\"Added artwork node: {manifest_label}\")\n",
        "\n",
        "        # Collect all creators/artists from metadata\n",
        "        creators = []\n",
        "        for item in manifest.get('metadata', []):\n",
        "            label = item.get('label', '').strip().lower()\n",
        "            value = item.get('value', '')\n",
        "            if label in ['creator', 'artist', 'author']:\n",
        "                creators.append(normalize_name(value))\n",
        "\n",
        "        # Create edges between each creator and the artwork\n",
        "        for creator in creators:\n",
        "            for artist_info in artists_info:\n",
        "                artist_names = [artist_info['preferred_name']] + artist_info['alternative_names']\n",
        "                normalized_artist_names = [name.lower() for name in artist_names]\n",
        "                if creator in normalized_artist_names:\n",
        "                    G.add_edge(artist_info['preferred_name'], manifest_label)\n",
        "                    logging.debug(f\"Created edge between '{artist_info['preferred_name']}' and '{manifest_label}' (Exact Match)\")\n",
        "                else:\n",
        "                    # Fuzzy matching\n",
        "                    for name in normalized_artist_names:\n",
        "                        similarity = fuzz.partial_ratio(name, creator)\n",
        "                        if similarity >= 85:\n",
        "                            G.add_edge(artist_info['preferred_name'], manifest_label)\n",
        "                            logging.debug(f\"Created edge between '{artist_info['preferred_name']}' and '{manifest_label}' (Fuzzy Match: {similarity})\")\n",
        "                            break  # Stop after first sufficient match\n",
        "\n",
        "    # Remove edges between artists if any (shouldn't happen)\n",
        "    for edge in list(G.edges()):\n",
        "        node1, node2 = edge\n",
        "        if G.nodes[node1].get('type') == 'artist' and G.nodes[node2].get('type') == 'artist':\n",
        "            G.remove_edge(node1, node2)\n",
        "            logging.debug(f\"Removed unintended edge between artists: {node1} - {node2}\")\n",
        "\n",
        "    # Layout\n",
        "    pos = nx.kamada_kawai_layout(G)\n",
        "\n",
        "    # Create edge traces\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    for edge in G.edges():\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "    edge_trace = go.Scatter(\n",
        "        x=edge_x, y=edge_y,\n",
        "        line=dict(width=2, color='#888'),\n",
        "        hoverinfo='none',\n",
        "        mode='lines'\n",
        "    )\n",
        "\n",
        "    # Create node traces\n",
        "    node_x = []\n",
        "    node_y = []\n",
        "    node_text = []\n",
        "    node_color = []\n",
        "    for node in G.nodes():\n",
        "        x, y = pos[node]\n",
        "        node_x.append(x)\n",
        "        node_y.append(y)\n",
        "        node_type = G.nodes[node].get('type', '')\n",
        "        if node_type == 'artist':\n",
        "            node_color.append('blue')\n",
        "            hover_text = f\"{node}<br>{G.nodes[node].get('description', '')}\"\n",
        "        else:\n",
        "            node_color.append('orange')\n",
        "            hover_text = node\n",
        "        node_text.append(hover_text)\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y,\n",
        "        mode='markers+text',\n",
        "        text=[node for node in G.nodes()],\n",
        "        textposition='top center',\n",
        "        hoverinfo='text',\n",
        "        hovertext=node_text,\n",
        "        marker=dict(\n",
        "            color=node_color,\n",
        "            size=20,\n",
        "            line_width=2\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Create the figure\n",
        "    fig = go.Figure(\n",
        "        data=[edge_trace, node_trace],\n",
        "        layout=go.Layout(\n",
        "            title='Artist and Artwork Relationships',\n",
        "            showlegend=False,\n",
        "            hovermode='closest',\n",
        "            margin=dict(b=20, l=5, r=5, t=40),\n",
        "            xaxis=dict(\n",
        "                showgrid=False,\n",
        "                showticklabels=False,\n",
        "                zeroline=False\n",
        "            ),\n",
        "            yaxis=dict(\n",
        "                showgrid=False,\n",
        "                showticklabels=False,\n",
        "                zeroline=False\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Display the figure explicitly with the 'colab' renderer\n",
        "    fig.show(renderer='colab')\n",
        "\n",
        "def validate_json(data, schema):\n",
        "    \"\"\"\n",
        "    Validate JSON data against a schema.\n",
        "\n",
        "    Args:\n",
        "        data (dict): JSON data to validate.\n",
        "        schema (dict): JSON schema.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if valid, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        validate(instance=data, schema=schema)\n",
        "        return True\n",
        "    except ValidationError as ve:\n",
        "        logging.error(f\"JSON validation error: {ve.message}\")\n",
        "        return False\n",
        "\n",
        "# ==============================\n",
        "# 8. User Interface Components\n",
        "# ==============================\n",
        "\n",
        "# Textarea for IIIF Manifest URIs\n",
        "iiif_manifest_uris_input = Textarea(\n",
        "    value='',\n",
        "    placeholder='Enter IIIF Manifest URIs (one per line)',\n",
        "    description='IIIF Manifests:',\n",
        "    layout=Layout(width='48%', height='200px')\n",
        ")\n",
        "\n",
        "# Textarea for Linked Data Entity URIs\n",
        "linked_data_uris_input = Textarea(\n",
        "    value='',\n",
        "    placeholder='Enter Linked Data Entity URIs (one per line)',\n",
        "    description='Linked Data URIs:',\n",
        "    layout=Layout(width='48%', height='200px')\n",
        ")\n",
        "\n",
        "# Slider for Fuzzy Matching Threshold\n",
        "fuzzy_threshold_slider = IntSlider(\n",
        "    value=85,\n",
        "    min=50,\n",
        "    max=100,\n",
        "    step=5,\n",
        "    description='Fuzzy Threshold:',\n",
        "    continuous_update=False,\n",
        "    readout=True,\n",
        "    readout_format='d',\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "\n",
        "# Button to Start Processing\n",
        "process_button = Button(\n",
        "    description='Process',\n",
        "    button_style='success',\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "\n",
        "# Output Area for Logs and Messages\n",
        "output_area = Output()\n",
        "\n",
        "# Progress Bar\n",
        "progress_bar = widgets.FloatProgress(\n",
        "    value=0.0,\n",
        "    min=0.0,\n",
        "    max=1.0,\n",
        "    step=0.01,\n",
        "    description='Progress:',\n",
        "    bar_style='info',\n",
        "    layout=Layout(width='100%')\n",
        ")\n",
        "\n",
        "# Summary HTML\n",
        "summary_html = HTML(\n",
        "    value=\"\",\n",
        "    placeholder='',\n",
        "    description='',\n",
        "    layout=Layout(width='100%')\n",
        ")\n",
        "\n",
        "# Display the UI\n",
        "display(\n",
        "    VBox([\n",
        "        HBox([iiif_manifest_uris_input, linked_data_uris_input]),\n",
        "        HBox([fuzzy_threshold_slider, process_button]),\n",
        "        progress_bar,\n",
        "        summary_html,\n",
        "        output_area\n",
        "    ])\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# 9. Main Processing Function\n",
        "# ==============================\n",
        "\n",
        "def process_uris(iiif_manifest_uris, linked_data_uris, fuzzy_threshold):\n",
        "    \"\"\"\n",
        "    Main function to process IIIF manifests and Linked Data URIs.\n",
        "\n",
        "    Args:\n",
        "        iiif_manifest_uris (list): List of IIIF Manifest URIs.\n",
        "        linked_data_uris (list): List of Linked Data Entity URIs.\n",
        "        fuzzy_threshold (int): Threshold for fuzzy matching.\n",
        "    \"\"\"\n",
        "    if not iiif_manifest_uris or not linked_data_uris:\n",
        "        logging.error(\"Please enter at least one IIIF manifest URI and one linked data entity URI.\")\n",
        "        return\n",
        "\n",
        "    total_steps = 4  # Fetch Linked Data, Fetch IIIF Manifests, Enrich Manifests, Create Visualization\n",
        "    current_step = 0\n",
        "    progress_bar.value = 0.0\n",
        "    summary_html.value = \"\"\n",
        "\n",
        "    # Update progress\n",
        "    def update_progress(step, message):\n",
        "        nonlocal current_step\n",
        "        current_step += 1\n",
        "        progress_bar.value = current_step / total_steps\n",
        "        summary_html.value += f\"<p><strong>{step}:</strong> {message}</p>\"\n",
        "\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        try:\n",
        "            # Step 1: Fetch Linked Data Entities\n",
        "            update_progress(\n",
        "                \"Fetching Linked Data Entities\",\n",
        "                f\"Starting to fetch {len(linked_data_uris)} linked data entities...\"\n",
        "            )\n",
        "            linked_data_entities = fetch_data_concurrently(linked_data_uris, fetch_linked_data)\n",
        "            logging.info(f\"Fetched {len(linked_data_entities)} linked data entities.\")\n",
        "            update_progress(\n",
        "                \"Fetching Linked Data Entities\",\n",
        "                f\"Successfully fetched {len(linked_data_entities)} linked data entities.\"\n",
        "            )\n",
        "\n",
        "            # Step 2: Fetch IIIF Manifests\n",
        "            update_progress(\n",
        "                \"Fetching IIIF Manifests\",\n",
        "                f\"Starting to fetch {len(iiif_manifest_uris)} IIIF manifests...\"\n",
        "            )\n",
        "            iiif_manifests = fetch_data_concurrently(iiif_manifest_uris, fetch_iiif_manifest)\n",
        "            logging.info(f\"Fetched {len(iiif_manifests)} IIIF manifests.\")\n",
        "            update_progress(\n",
        "                \"Fetching IIIF Manifests\",\n",
        "                f\"Successfully fetched {len(iiif_manifests)} IIIF manifests.\"\n",
        "            )\n",
        "\n",
        "            if not linked_data_entities:\n",
        "                logging.error(\"No linked data entities fetched successfully. Aborting process.\")\n",
        "                update_progress(\"Error\", \"No linked data entities fetched successfully. Aborting process.\")\n",
        "                return\n",
        "            if not iiif_manifests:\n",
        "                logging.error(\"No IIIF manifests fetched successfully. Aborting process.\")\n",
        "                update_progress(\"Error\", \"No IIIF manifests fetched successfully. Aborting process.\")\n",
        "                return\n",
        "\n",
        "            # Step 3: Extract Artist Information\n",
        "            update_progress(\n",
        "                \"Extracting Artist Information\",\n",
        "                \"Extracting artist information from linked data entities...\"\n",
        "            )\n",
        "            artists_info = [extract_artist_info(entity) for entity in linked_data_entities]\n",
        "            logging.info(f\"Extracted information for {len(artists_info)} artists.\")\n",
        "            update_progress(\n",
        "                \"Extracting Artist Information\",\n",
        "                f\"Extracted information for {len(artists_info)} artists.\"\n",
        "            )\n",
        "\n",
        "            # Step 4: Enrich IIIF Manifests\n",
        "            update_progress(\n",
        "                \"Enriching IIIF Manifests\",\n",
        "                \"Enriching IIIF manifests with artist information...\"\n",
        "            )\n",
        "            enriched_manifests = enrich_iiif_manifests(iiif_manifests, artists_info, fuzzy_threshold)\n",
        "            logging.info(f\"Enriched {len(enriched_manifests)} IIIF manifests.\")\n",
        "            update_progress(\n",
        "                \"Enriching IIIF Manifests\",\n",
        "                f\"Enriched {len(enriched_manifests)} IIIF manifests.\"\n",
        "            )\n",
        "\n",
        "            # Step 5: Save Enriched Manifests\n",
        "            update_progress(\n",
        "                \"Saving Enriched Manifests\",\n",
        "                \"Saving enriched IIIF manifests to JSON files...\"\n",
        "            )\n",
        "            for idx, manifest in enumerate(enriched_manifests):\n",
        "                filename = f'enriched_manifest_{idx+1}.json'\n",
        "                try:\n",
        "                    with open(filename, 'w', encoding='utf-8') as file:\n",
        "                        json.dump(manifest, file, ensure_ascii=False, indent=2)\n",
        "                    logging.info(f\"Enriched manifest saved to {filename}\")\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Failed to save enriched manifest to {filename}: {e}\")\n",
        "            update_progress(\n",
        "                \"Saving Enriched Manifests\",\n",
        "                f\"Saved {len(enriched_manifests)} enriched IIIF manifests.\"\n",
        "            )\n",
        "\n",
        "            # Step 6: Create Visualization\n",
        "            update_progress(\n",
        "                \"Creating Visualization\",\n",
        "                \"Generating relationship graph...\"\n",
        "            )\n",
        "            create_relationship_graph(enriched_manifests, artists_info)\n",
        "            update_progress(\n",
        "                \"Creating Visualization\",\n",
        "                \"Relationship graph generated successfully.\"\n",
        "            )\n",
        "\n",
        "            # Completion Message\n",
        "            summary_html.value += \"<p><strong>Process Completed Successfully!</strong></p>\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An unexpected error occurred: {e}\")\n",
        "            summary_html.value += f\"<p style='color:red;'><strong>Error:</strong> {e}</p>\"\n",
        "\n",
        "# ==============================\n",
        "# 10. Link the Button to the Callback Function\n",
        "# ==============================\n",
        "\n",
        "def on_process_button_clicked(b):\n",
        "    \"\"\"Callback function triggered when the process button is clicked.\"\"\"\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        # Extract URIs from input fields\n",
        "        iiif_manifest_uris = [\n",
        "            uri.strip() for uri in iiif_manifest_uris_input.value.strip().split('\\n')\n",
        "            if uri.strip()\n",
        "        ]\n",
        "        linked_data_uris = [\n",
        "            uri.strip() for uri in linked_data_uris_input.value.strip().split('\\n')\n",
        "            if uri.strip()\n",
        "        ]\n",
        "        fuzzy_threshold = fuzzy_threshold_slider.value\n",
        "        logging.info(\"Starting the processing of URIs...\")\n",
        "        process_uris(iiif_manifest_uris, linked_data_uris, fuzzy_threshold)\n",
        "\n",
        "process_button.on_click(on_process_button_clicked)\n"
      ]
    }
  ]
}