{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOnlxP8hoqEa5HIjJp07MvD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsandaver/hsandaver/blob/main/entity_extractor_wikipedia_experimental_entity_picker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM-CldL8H1d7"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", package])\n",
        "\n",
        "# Install required packages\n",
        "install(\"pymupdf\")\n",
        "install(\"spacy\")\n",
        "install(\"SPARQLWrapper\")\n",
        "install(\"pandas\")\n",
        "install(\"tqdm\")\n",
        "install(\"requests\")\n",
        "\n",
        "# Import libraries\n",
        "import spacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from google.colab import files\n",
        "from IPython.display import display  # For displaying DataFrame\n",
        "from tqdm import tqdm  # Progress bar\n",
        "import re\n",
        "from collections import Counter\n",
        "import requests\n",
        "\n",
        "# Function to set up spaCy NLP model with EntityRuler\n",
        "def setup_nlp():\n",
        "    # Download the spaCy model if not already present\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_md\")  # Using medium model for better accuracy\n",
        "    except OSError:\n",
        "        print(\"Downloading 'en_core_web_md' model...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_md\"])\n",
        "        nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "    # Initialize the EntityRuler and add it to the pipeline before the NER component\n",
        "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "\n",
        "    # Define patterns to exclude (e.g., titles like Mr., Ms., Dr.)\n",
        "    patterns = [\n",
        "        {\"label\": \"TITLE\", \"pattern\": \"Mr.\"},\n",
        "        {\"label\": \"TITLE\", \"pattern\": \"Ms.\"},\n",
        "        {\"label\": \"TITLE\", \"pattern\": \"Dr.\"},\n",
        "        {\"label\": \"TITLE\", \"pattern\": \"Prof.\"},\n",
        "        {\"label\": \"TITLE\", \"pattern\": \"Sir\"},\n",
        "        {\"label\": \"TITLE\", \"pattern\": \"Lady\"},\n",
        "        # Add more patterns as needed\n",
        "    ]\n",
        "\n",
        "    # Add patterns to the EntityRuler\n",
        "    ruler.add_patterns(patterns)\n",
        "\n",
        "    return nlp\n",
        "\n",
        "# Initialize spaCy NLP model\n",
        "nlp = setup_nlp()\n",
        "\n",
        "# Function to upload PDF in Colab\n",
        "def upload_pdf():\n",
        "    print(\"Please upload your PDF file.\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded. Exiting.\")\n",
        "        sys.exit()\n",
        "    pdf_path = next(iter(uploaded))\n",
        "    return pdf_path\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(doc):\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)  # Load page\n",
        "        page_text = page.get_text()  # Extract text from page\n",
        "        text += page_text + \"\\n\"  # Add newline for separation between pages\n",
        "    return text\n",
        "\n",
        "# Function to extract contextual keywords (optional, advanced)\n",
        "def extract_contextual_keywords(text, nlp_model):\n",
        "    \"\"\"\n",
        "    Extract contextual keywords related to person entities.\n",
        "\n",
        "    :param text: Extracted text from PDF\n",
        "    :param nlp_model: Loaded spaCy NLP model\n",
        "    :return: Dictionary mapping entity names to lists of contextual keywords\n",
        "    \"\"\"\n",
        "    doc = nlp_model(text)\n",
        "    context = {}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            # Extract the sentence containing the entity as a Span object\n",
        "            sentence = ent.sent\n",
        "            # Extract nouns and adjectives as potential contextual keywords\n",
        "            keywords = [token.lemma_ for token in sentence if token.pos_ in ['NOUN', 'ADJ']]\n",
        "            context.setdefault(ent.text.strip(), []).extend(keywords)\n",
        "\n",
        "    # Remove duplicates\n",
        "    for key in context:\n",
        "        context[key] = list(set(context[key]))\n",
        "\n",
        "    return context\n",
        "\n",
        "# Function to extract person entities with filtering\n",
        "def extract_person_entities(text, nlp_model):\n",
        "    doc_nlp = nlp_model(text)\n",
        "    # Extract entities labeled as PERSON\n",
        "    person_entities = [ent.text.strip() for ent in doc_nlp.ents if ent.label_ == \"PERSON\"]\n",
        "\n",
        "    print(\"\\n=== All Extracted Person Entities ===\")\n",
        "    for idx, ent in enumerate(person_entities, 1):\n",
        "        print(f\"{idx}. {ent}\")\n",
        "\n",
        "    # Count the frequency of each entity\n",
        "    entity_counts = Counter(person_entities)\n",
        "\n",
        "    # Define a regex pattern for valid names (allowing apostrophes and more)\n",
        "    pattern = re.compile(r'^[A-Za-z\\s\\-\\.\\']+$')\n",
        "\n",
        "    # Define a blacklist of common false positives\n",
        "    blacklist = {'John Doe', 'Jane Smith', 'Mr.', 'Ms.', 'Dr.', 'Prof.', 'Sir', 'Lady'}  # Extend as needed\n",
        "\n",
        "    # Filter entities:\n",
        "    # - Appear at least once\n",
        "    # - Match the regex pattern\n",
        "    # - Not in the blacklist\n",
        "    # - Length greater than 1\n",
        "    filtered_entities = [\n",
        "        ent for ent, count in entity_counts.items()\n",
        "        if count >= 1 and pattern.match(ent) and ent not in blacklist and len(ent) > 1\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== Filtered Person Entities ===\")\n",
        "    for idx, ent in enumerate(filtered_entities, 1):\n",
        "        print(f\"{idx}. {ent}\")\n",
        "\n",
        "    return list(set(filtered_entities))\n",
        "\n",
        "# Function to search Wikidata using the Search API\n",
        "def search_wikidata_api(entity):\n",
        "    \"\"\"\n",
        "    Search Wikidata using the Search API.\n",
        "\n",
        "    :param entity: Name of the entity to search\n",
        "    :return: List of search results with labels and descriptions\n",
        "    \"\"\"\n",
        "    url = \"https://www.wikidata.org/w/api.php\"\n",
        "    params = {\n",
        "        'action': 'wbsearchentities',\n",
        "        'search': entity,\n",
        "        'language': 'en',\n",
        "        'format': 'json',\n",
        "        'limit': 10  # Increased limit to 10\n",
        "    }\n",
        "    print(f\"\\nSearching Wikidata for: '{entity}' with limit=10\")\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: Received status code {response.status_code} from Wikidata API.\")\n",
        "        return []\n",
        "    data = response.json()\n",
        "    return data.get('search', [])\n",
        "\n",
        "# Function to fetch entity details by Wikidata ID\n",
        "def fetch_entity_by_id(wikidata_id):\n",
        "    \"\"\"\n",
        "    Fetch entity details from Wikidata by ID.\n",
        "\n",
        "    :param wikidata_id: Wikidata ID (e.g., Q42)\n",
        "    :return: Dictionary with 'Wikidata ID', 'Label', 'Description'\n",
        "    \"\"\"\n",
        "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{wikidata_id}.json\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: Received status code {response.status_code} when fetching entity {wikidata_id}.\")\n",
        "        return {\n",
        "            \"Wikidata ID\": wikidata_id,\n",
        "            \"Label\": \"N/A\",\n",
        "            \"Description\": \"N/A\"\n",
        "        }\n",
        "    data = response.json()\n",
        "    try:\n",
        "        entity = data['entities'][wikidata_id]\n",
        "        label = entity['labels']['en']['value']\n",
        "        description = entity['descriptions']['en']['value']\n",
        "        return {\n",
        "            \"Wikidata ID\": wikidata_id,\n",
        "            \"Label\": label,\n",
        "            \"Description\": description\n",
        "        }\n",
        "    except KeyError:\n",
        "        return {\n",
        "            \"Wikidata ID\": wikidata_id,\n",
        "            \"Label\": \"N/A\",\n",
        "            \"Description\": \"N/A\"\n",
        "        }\n",
        "\n",
        "# Function to rank entities based on contextual keywords\n",
        "def rank_entities(entities, keywords):\n",
        "    \"\"\"\n",
        "    Rank entities based on the number of keyword matches in their descriptions.\n",
        "\n",
        "    :param entities: List of entities (dictionaries with 'Description')\n",
        "    :param keywords: List of keywords to match against descriptions\n",
        "    :return: The entity with the highest keyword match count\n",
        "    \"\"\"\n",
        "    if not keywords:\n",
        "        # If no keywords provided, return None\n",
        "        return None\n",
        "\n",
        "    ranked_entities = []\n",
        "    for entity in entities:\n",
        "        description = entity.get(\"Description\", \"\").lower()\n",
        "        match_count = sum(keyword.lower() in description for keyword in keywords)\n",
        "        ranked_entities.append((match_count, entity))\n",
        "\n",
        "    # Sort entities by match_count descending\n",
        "    ranked_entities.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    if ranked_entities and ranked_entities[0][0] > 0:\n",
        "        # Return the entity with the highest match count\n",
        "        return ranked_entities[0][1]\n",
        "    else:\n",
        "        # No matching keywords found\n",
        "        return None\n",
        "\n",
        "# Main processing function\n",
        "def process_pdf():\n",
        "    pdf_path = upload_pdf()\n",
        "    print(f\"\\nProcessing PDF: {pdf_path}\")\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening PDF: {e}\")\n",
        "        sys.exit()\n",
        "\n",
        "    pdf_text = extract_text_from_pdf(doc)\n",
        "    print(\"\\n=== Extracted Text from PDF ===\")\n",
        "    # Display the first 1000 characters to avoid flooding the output\n",
        "    print(pdf_text[:1000] + \"...\" if len(pdf_text) > 1000 else pdf_text)\n",
        "\n",
        "    person_entities = extract_person_entities(pdf_text, nlp)\n",
        "    print(f\"\\nFound {len(person_entities)} unique person entities.\")\n",
        "\n",
        "    if not person_entities:\n",
        "        print(\"No person entities found in the PDF.\")\n",
        "        sys.exit()\n",
        "\n",
        "    # Display the list of extracted person entities\n",
        "    print(\"\\n=== Extracted Person Entities ===\")\n",
        "    for idx, entity in enumerate(person_entities, 1):\n",
        "        print(f\"{idx}. {entity}\")\n",
        "\n",
        "    # Optional: Allow user to inspect entities before querying\n",
        "    user_input = input(\"\\nDo you want to proceed with querying Wikidata for these entities? (yes/no): \").strip().lower()\n",
        "    if user_input not in ['yes', 'y']:\n",
        "        print(\"Operation cancelled by the user.\")\n",
        "        sys.exit()\n",
        "\n",
        "    # Extract contextual keywords (optional, advanced)\n",
        "    contextual_keywords = extract_contextual_keywords(pdf_text, nlp)\n",
        "\n",
        "    # Initialize cache to store previously searched entities\n",
        "    entity_cache = {}\n",
        "\n",
        "    # Query Wikidata for each person entity and collect data with progress bar\n",
        "    entity_data = []\n",
        "    for entity in tqdm(person_entities, desc=\"Processing Entities\", unit=\"entity\"):\n",
        "        print(f\"\\nProcessing entity: '{entity}'\")\n",
        "\n",
        "        # Check cache first\n",
        "        if entity in entity_cache:\n",
        "            print(f\"Retrieving cached results for entity: '{entity}'\")\n",
        "            search_results = entity_cache[entity]\n",
        "        else:\n",
        "            search_results = search_wikidata_api(entity)\n",
        "            # Store in cache\n",
        "            entity_cache[entity] = search_results\n",
        "\n",
        "        if not search_results:\n",
        "            # No entities found, append N/A\n",
        "            entity_data.append({\n",
        "                \"Name\": entity,\n",
        "                \"Wikidata ID\": \"N/A\",\n",
        "                \"Label\": \"N/A\",\n",
        "                \"Description\": \"N/A\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # If only one entity found, select it automatically\n",
        "        if len(search_results) == 1:\n",
        "            selected_entity = {\n",
        "                \"Wikidata ID\": search_results[0]['id'],\n",
        "                \"Label\": search_results[0]['label'],\n",
        "                \"Description\": search_results[0].get('description', 'N/A')\n",
        "            }\n",
        "            print(f\"Automatically selected: {selected_entity['Label']} - {selected_entity['Description']}\")\n",
        "        else:\n",
        "            # Multiple entities found, prompt user to select\n",
        "            print(f\"Multiple Wikidata entries found for '{entity}':\")\n",
        "            for idx, ent in enumerate(search_results, 1):\n",
        "                print(f\"{idx}. {ent['label']} - {ent.get('description', 'N/A')}\")\n",
        "\n",
        "            # Prompt user to select the correct entity\n",
        "            while True:\n",
        "                try:\n",
        "                    selection = int(input(f\"Select the correct entity for '{entity}' (1-{len(search_results)}), or 0 to skip: \"))\n",
        "                    if selection == 0:\n",
        "                        selected_entity = None\n",
        "                        print(f\"Skipping entity: '{entity}'\")\n",
        "                        break\n",
        "                    elif 1 <= selection <= len(search_results):\n",
        "                        selected_entity = {\n",
        "                            \"Wikidata ID\": search_results[selection - 1]['id'],\n",
        "                            \"Label\": search_results[selection - 1]['label'],\n",
        "                            \"Description\": search_results[selection - 1].get('description', 'N/A')\n",
        "                        }\n",
        "                        print(f\"Selected: {selected_entity['Label']} - {selected_entity['Description']}\")\n",
        "                        break\n",
        "                    else:\n",
        "                        print(f\"Please enter a number between 0 and {len(search_results)}.\")\n",
        "                except ValueError:\n",
        "                    print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "        if selected_entity:\n",
        "            entity_data.append({\n",
        "                \"Name\": entity,\n",
        "                \"Wikidata ID\": selected_entity[\"Wikidata ID\"],\n",
        "                \"Label\": selected_entity[\"Label\"],\n",
        "                \"Description\": selected_entity[\"Description\"]\n",
        "            })\n",
        "        else:\n",
        "            entity_data.append({\n",
        "                \"Name\": entity,\n",
        "                \"Wikidata ID\": \"N/A\",\n",
        "                \"Label\": \"N/A\",\n",
        "                \"Description\": \"N/A\"\n",
        "            })\n",
        "        # Optional: Delay to respect rate limits\n",
        "        # import time\n",
        "        # time.sleep(1)  # Sleep for 1 second\n",
        "\n",
        "    # Convert the results to a pandas DataFrame for display\n",
        "    df = pd.DataFrame(entity_data)\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(\"\\n=== Person Entities Extracted ===\")\n",
        "    display(df)\n",
        "\n",
        "    # Optionally, allow the user to download the DataFrame as a CSV\n",
        "    try:\n",
        "        csv = df.to_csv(index=False)\n",
        "        with open(\"person_entities.csv\", \"w\", encoding='utf-8') as f:\n",
        "            f.write(csv)\n",
        "        print(\"\\nDownloading 'person_entities.csv'...\")\n",
        "        files.download('person_entities.csv')\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading CSV: {e}\")\n",
        "\n",
        "# Execute the main processing function\n",
        "process_pdf()"
      ]
    }
  ]
}