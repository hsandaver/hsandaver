{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMAHwtOgDbayhRDUBgEfPaO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsandaver/hsandaver/blob/main/PromptGenerator1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7E6W5-vB4ZA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Attempt to import Google Colab's files module; handle if not in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Download necessary NLTK data if not already available\n",
        "nltk_packages = ['wordnet', 'omw-1.4', 'punkt', 'averaged_perceptron_tagger', 'stopwords']\n",
        "for package in nltk_packages:\n",
        "    nltk.download(package, quiet=True)\n",
        "\n",
        "# Mapping NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"\n",
        "    Map POS tag to the format accepted by wordnet.synsets()\n",
        "    \"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None  # Return None if POS tag is not recognized\n",
        "\n",
        "# Function to load the dataset\n",
        "def load_dataset(default_file='your_dataset.jsonl'):\n",
        "    \"\"\"\n",
        "    Load dataset from a JSON Lines file.\n",
        "    If the file is not found locally, prompt the user to upload it (Colab only).\n",
        "    \"\"\"\n",
        "    if os.path.exists(default_file):\n",
        "        file_path = default_file\n",
        "    else:\n",
        "        if IN_COLAB:\n",
        "            print(f\"File '{default_file}' not found locally. Please upload the dataset.\")\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                raise FileNotFoundError(\"No file uploaded.\")\n",
        "            file_path = list(uploaded.keys())[0]\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File '{default_file}' not found and not running in Colab.\")\n",
        "\n",
        "    dataset = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    try:\n",
        "                        dataset.append(json.loads(line))\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        logging.warning(f\"Error decoding JSON: {e} in line: {line}\")\n",
        "        if not dataset:\n",
        "            raise ValueError(\"Dataset is empty. Please check the file contents.\")\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred while loading the dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Remove punctuation and tokenize the text into words.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if re.match(r'\\w+', token)]  # Keep words only\n",
        "    return tokens\n",
        "\n",
        "# Function to extract unique elements from the dataset\n",
        "def extract_elements_from_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Extract unique subjects, settings, moods, lighting, and perspectives from the dataset.\n",
        "    \"\"\"\n",
        "    features = defaultdict(set)\n",
        "\n",
        "    # Define patterns for extraction (can be customized based on dataset)\n",
        "    patterns = {\n",
        "        'subjects': re.compile(r'\\b\\w+\\s+eyes\\b', re.IGNORECASE),\n",
        "        'settings': re.compile(r'\\bforest\\b|\\bocean\\b|\\bsea\\b|\\bcity\\b|\\bmountain\\b|\\bdesert\\b|\\bbeach\\b', re.IGNORECASE),\n",
        "        'moods': re.compile(r'\\bintrospective\\b|\\bserene\\b|\\bwistful\\b|\\breflective\\b|\\badventurous\\b|\\bjoyful\\b|\\bsomber\\b', re.IGNORECASE),\n",
        "        'lighting': re.compile(r'\\bsoft\\b|\\bgolden[-\\s]hour\\b|\\bsunset\\b|\\bmoonlight\\b|\\bneon\\b|\\bharsh\\b|\\bdim\\b', re.IGNORECASE),\n",
        "        'perspectives': re.compile(r'\\blow-angle\\b|\\bbug’s-eye\\b|\\bthree-quarter view\\b|\\bclose-up\\b|\\bwide-angle\\b|\\bbird’s-eye view\\b', re.IGNORECASE)\n",
        "    }\n",
        "\n",
        "    for entry in dataset:\n",
        "        prompt = entry.get('prompt', '')\n",
        "        if not prompt:\n",
        "            continue\n",
        "        for key, pattern in patterns.items():\n",
        "            matches = pattern.findall(prompt)\n",
        "            for match in matches:\n",
        "                features[key].add(match.lower())\n",
        "\n",
        "    # Convert sets to lists\n",
        "    for key in features:\n",
        "        features[key] = list(features[key])\n",
        "\n",
        "    return features\n",
        "\n",
        "# Function to combine base prompt with additional elements\n",
        "def combine_elements(base_prompt, additional_elements):\n",
        "    \"\"\"\n",
        "    Incorporate additional elements into the base prompt to create a more detailed prompt.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parts = [base_prompt]\n",
        "        additional_subject = random.choice(additional_elements['subjects']) if additional_elements['subjects'] else ''\n",
        "        additional_setting = random.choice(additional_elements['settings']) if additional_elements['settings'] else ''\n",
        "        additional_mood = random.choice(additional_elements['moods']) if additional_elements['moods'] else ''\n",
        "        additional_lighting = random.choice(additional_elements['lighting']) if additional_elements['lighting'] else ''\n",
        "        additional_perspective = random.choice(additional_elements['perspectives']) if additional_elements['perspectives'] else ''\n",
        "\n",
        "        if additional_subject:\n",
        "            parts.append(f\"The subject has {additional_subject}\")\n",
        "        if additional_setting:\n",
        "            parts.append(f\"set against a {additional_setting} backdrop\")\n",
        "        if additional_mood:\n",
        "            parts.append(f\"The mood is {additional_mood}\")\n",
        "        if additional_lighting:\n",
        "            parts.append(f\"illuminated by {additional_lighting} lighting\")\n",
        "        if additional_perspective:\n",
        "            parts.append(f\"and the portrait is captured from a {additional_perspective} perspective.\")\n",
        "\n",
        "        new_prompt = '. '.join(filter(None, parts))\n",
        "        return new_prompt\n",
        "    except KeyError as e:\n",
        "        logging.warning(f\"Missing feature category: {e}\")\n",
        "        return base_prompt\n",
        "\n",
        "# Cache for synonyms to improve performance\n",
        "synonym_cache = {}\n",
        "\n",
        "# Function to find and replace words with synonyms for added creativity\n",
        "def replace_with_synonyms(prompt):\n",
        "    \"\"\"\n",
        "    Replace adjectives and adverbs in the prompt with their synonyms to add variety.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(prompt)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    new_tokens = []\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for word, tag in tagged_tokens:\n",
        "        wn_pos = get_wordnet_pos(tag)\n",
        "        # Only replace adjectives and adverbs\n",
        "        if wn_pos not in (wordnet.ADJ, wordnet.ADV):\n",
        "            new_tokens.append(word)\n",
        "            continue\n",
        "\n",
        "        # Exclude stopwords and function words\n",
        "        if word.lower() in stop_words:\n",
        "            new_tokens.append(word)\n",
        "            continue\n",
        "\n",
        "        word_lower = word.lower()\n",
        "        # Check cache first\n",
        "        if word_lower in synonym_cache:\n",
        "            synonyms = synonym_cache[word_lower]\n",
        "        else:\n",
        "            synonyms = wordnet.synsets(word_lower, pos=wn_pos)\n",
        "            synonyms = [lemma.name().replace('_', ' ') for syn in synonyms for lemma in syn.lemmas()]\n",
        "            # Filter to single-word synonyms that are alphabetic\n",
        "            synonyms = [syn for syn in synonyms if ' ' not in syn and syn.isalpha()]\n",
        "            synonym_cache[word_lower] = synonyms\n",
        "\n",
        "        # Exclude the original word and select a random synonym\n",
        "        synonyms = [syn for syn in synonyms if syn.lower() != word_lower]\n",
        "        if synonyms:\n",
        "            synonym = random.choice(synonyms)\n",
        "            # Preserve the original casing\n",
        "            if word.isupper():\n",
        "                synonym = synonym.upper()\n",
        "            elif word[0].isupper():\n",
        "                synonym = synonym.capitalize()\n",
        "            new_tokens.append(synonym)\n",
        "        else:\n",
        "            new_tokens.append(word)\n",
        "\n",
        "    return ' '.join(new_tokens)\n",
        "\n",
        "# Function to truncate the prompt to a given token limit\n",
        "def truncate_prompt(prompt, token_limit):\n",
        "    \"\"\"\n",
        "    Truncate the prompt to the specified number of tokens without breaking words.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(prompt)\n",
        "    if len(tokens) <= token_limit:\n",
        "        return prompt\n",
        "    truncated_tokens = tokens[:token_limit]\n",
        "    # Ensure that the prompt ends gracefully\n",
        "    truncated_prompt = ' '.join(truncated_tokens)\n",
        "    if not truncated_prompt.endswith('.'):\n",
        "        truncated_prompt += '...'\n",
        "    return truncated_prompt\n",
        "\n",
        "# Function to generate a creative random prompt based on dataset elements\n",
        "def generate_prompt(dataset, additional_elements, token_limit=75):\n",
        "    \"\"\"\n",
        "    Generate a creative prompt by combining elements from the dataset and adding synonyms.\n",
        "    \"\"\"\n",
        "    base_entry = random.choice(dataset)\n",
        "    base_prompt = base_entry.get('prompt', 'A creative scene.')\n",
        "\n",
        "    combined_prompt = combine_elements(base_prompt, additional_elements)\n",
        "    combined_prompt = replace_with_synonyms(combined_prompt)\n",
        "\n",
        "    if len(tokenize(combined_prompt)) > token_limit:\n",
        "        combined_prompt = truncate_prompt(combined_prompt, token_limit)\n",
        "\n",
        "    return combined_prompt\n",
        "\n",
        "# Function to save the generated prompts to a file\n",
        "def save_generated_prompts(prompts, output_file):\n",
        "    \"\"\"\n",
        "    Save the list of generated prompts to a text file, each on a new line.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as file:\n",
        "            for prompt in prompts:\n",
        "                file.write(f\"{prompt}\\n\")\n",
        "        logging.info(f\"{len(prompts)} prompts generated and saved to '{output_file}'.\")\n",
        "\n",
        "        if IN_COLAB:\n",
        "            files.download(output_file)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred while saving prompts: {e}\")\n",
        "        raise\n",
        "\n",
        "# Main function to run the generator\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the prompt generation process.\n",
        "    \"\"\"\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Generate creative prompts based on a dataset.\")\n",
        "    parser.add_argument('--dataset', type=str, default='your_dataset.jsonl', help='Path to the dataset JSONL file.')\n",
        "    parser.add_argument('--output', type=str, default='generated_prompts.txt', help='Output file for generated prompts.')\n",
        "    parser.add_argument('--number', type=int, default=10, help='Number of prompts to generate.')\n",
        "    parser.add_argument('--tokens', type=int, default=75, help='Maximum number of tokens per prompt.')\n",
        "\n",
        "    # Use parse_known_args() instead of parse_args()\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    try:\n",
        "        dataset = load_dataset(args.dataset)\n",
        "    except Exception as e:\n",
        "        logging.error(e)\n",
        "        return\n",
        "\n",
        "    additional_elements = extract_elements_from_dataset(dataset)\n",
        "\n",
        "    if not any(additional_elements.values()):\n",
        "        logging.error(\"No additional elements extracted. Please check the dataset's 'prompt' fields.\")\n",
        "        return\n",
        "\n",
        "    generated_prompts = []\n",
        "    for _ in range(args.number):\n",
        "        generated_prompt = generate_prompt(dataset, additional_elements, token_limit=args.tokens)\n",
        "        generated_prompts.append(generated_prompt)\n",
        "\n",
        "    try:\n",
        "        save_generated_prompts(generated_prompts, args.output)\n",
        "    except Exception as e:\n",
        "        logging.error(e)\n",
        "        return\n",
        "\n",
        "# Entry point\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}